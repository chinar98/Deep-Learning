{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the Digits Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits is a dataset of handwritten digits. <BR>\n",
    "Each feature is the intensity of one pixel of an 8 x 8 image.<BR>\n",
    "Therefore there are 64 intensity related features.<BR>\n",
    "The sklearn library provides an inbuilt function load_digits(), to load this dataset.<BR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Create feature matrix\n",
    "X = digits.data\n",
    "\n",
    "# Create target vector\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the number of samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that there is data for  1797 images and 64 features for each image.<br> \n",
    "View the feature values of a tuple<br> There are 64 values in each tuple, as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first observation's feature values\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the actual image from characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now view the actual image by using a gray scale graph and the given 64 features<br>\n",
    "The matshow() method takes an input array and displays it in the given color scale<br>\n",
    "The digits in sklearn dataset are of much lower resolution than the original MNIST datset images which were 28Ã—28 pixels.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the observation's feature values as an image\n",
    "def showdigit(i):\n",
    "    #We first set the colomap to grayscale\n",
    "    plt.gray() \n",
    "    plt.figure(figsize = [1,1])\n",
    "\n",
    "    #Choose the observation/tuple to be displayed.\n",
    "    plt.matshow(digits.images[i]) \n",
    "\n",
    "    #Show the grayscale plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 72x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showdigit(325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(i):\n",
    "    '''Plots 16 digits, starting with digit i'''\n",
    "    nplots = 9\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    for j in range(nplots):\n",
    "        plt.subplot(3,3,j+1)\n",
    "        plt.imshow(digits.images[i+j], cmap='binary')\n",
    "        plt.title(digits.target[i+j])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multi(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design our basic MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detine a Multi-Layer-Perceptron with just one hidden layer<br>\n",
    "We therefore have 3 layers : <br>\n",
    "Input Layer : Our images have 64 attributes or features. Hence our input layer will have 64 neurons<br>\n",
    "Output Layer: We expect a 0-9 digits classification. So we can have 10 neurons in the output, one for each output.<br>\n",
    "Hidden Layer: We can arbitrarily choose the number of neurons in this layer. TO achieve a dimension reduction here , we use say, 15 neurons. <br>\n",
    "<br>\n",
    "Also, we will build a dense of FULLY CONNECTED neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test and Train Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 64)\n"
     ]
    }
   ],
   "source": [
    "print( X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "print( y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True,max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15249593\n",
      "Iteration 2, loss = 1.68980144\n",
      "Iteration 3, loss = 1.25918855\n",
      "Iteration 4, loss = 0.90101278\n",
      "Iteration 5, loss = 0.64705257\n",
      "Iteration 6, loss = 0.50130506\n",
      "Iteration 7, loss = 0.39892879\n",
      "Iteration 8, loss = 0.34805643\n",
      "Iteration 9, loss = 0.28101277\n",
      "Iteration 10, loss = 0.25480912\n",
      "Iteration 11, loss = 0.21190128\n",
      "Iteration 12, loss = 0.20743182\n",
      "Iteration 13, loss = 0.18213260\n",
      "Iteration 14, loss = 0.15995221\n",
      "Iteration 15, loss = 0.15057888\n",
      "Iteration 16, loss = 0.13909420\n",
      "Iteration 17, loss = 0.12906573\n",
      "Iteration 18, loss = 0.11858621\n",
      "Iteration 19, loss = 0.11661341\n",
      "Iteration 20, loss = 0.10772064\n",
      "Iteration 21, loss = 0.09977674\n",
      "Iteration 22, loss = 0.09247116\n",
      "Iteration 23, loss = 0.09095041\n",
      "Iteration 24, loss = 0.08713646\n",
      "Iteration 25, loss = 0.08637697\n",
      "Iteration 26, loss = 0.08517056\n",
      "Iteration 27, loss = 0.07921742\n",
      "Iteration 28, loss = 0.08041866\n",
      "Iteration 29, loss = 0.07065617\n",
      "Iteration 30, loss = 0.06604370\n",
      "Iteration 31, loss = 0.07067165\n",
      "Iteration 32, loss = 0.06156586\n",
      "Iteration 33, loss = 0.05875288\n",
      "Iteration 34, loss = 0.06791662\n",
      "Iteration 35, loss = 0.05789268\n",
      "Iteration 36, loss = 0.05662516\n",
      "Iteration 37, loss = 0.05488521\n",
      "Iteration 38, loss = 0.05053356\n",
      "Iteration 39, loss = 0.04960683\n",
      "Iteration 40, loss = 0.04874293\n",
      "Iteration 41, loss = 0.04558730\n",
      "Iteration 42, loss = 0.04431762\n",
      "Iteration 43, loss = 0.04342598\n",
      "Iteration 44, loss = 0.04196795\n",
      "Iteration 45, loss = 0.04145282\n",
      "Iteration 46, loss = 0.04073692\n",
      "Iteration 47, loss = 0.03931750\n",
      "Iteration 48, loss = 0.03869286\n",
      "Iteration 49, loss = 0.03787976\n",
      "Iteration 50, loss = 0.03731483\n",
      "Iteration 51, loss = 0.03616545\n",
      "Iteration 52, loss = 0.03611156\n",
      "Iteration 53, loss = 0.03568974\n",
      "Iteration 54, loss = 0.03529782\n",
      "Iteration 55, loss = 0.03719276\n",
      "Iteration 56, loss = 0.03509162\n",
      "Iteration 57, loss = 0.03537342\n",
      "Iteration 58, loss = 0.03357963\n",
      "Iteration 59, loss = 0.03225060\n",
      "Iteration 60, loss = 0.03177173\n",
      "Iteration 61, loss = 0.03103484\n",
      "Iteration 62, loss = 0.03060679\n",
      "Iteration 63, loss = 0.03023619\n",
      "Iteration 64, loss = 0.03057684\n",
      "Iteration 65, loss = 0.02966342\n",
      "Iteration 66, loss = 0.02916946\n",
      "Iteration 67, loss = 0.02843174\n",
      "Iteration 68, loss = 0.02802842\n",
      "Iteration 69, loss = 0.02797680\n",
      "Iteration 70, loss = 0.02758983\n",
      "Iteration 71, loss = 0.02706073\n",
      "Iteration 72, loss = 0.02662779\n",
      "Iteration 73, loss = 0.02632751\n",
      "Iteration 74, loss = 0.02574541\n",
      "Iteration 75, loss = 0.02531232\n",
      "Iteration 76, loss = 0.02500393\n",
      "Iteration 77, loss = 0.02480064\n",
      "Iteration 78, loss = 0.02446753\n",
      "Iteration 79, loss = 0.02417807\n",
      "Iteration 80, loss = 0.02405199\n",
      "Iteration 81, loss = 0.02392718\n",
      "Iteration 82, loss = 0.02332192\n",
      "Iteration 83, loss = 0.02333891\n",
      "Iteration 84, loss = 0.02289076\n",
      "Iteration 85, loss = 0.02265835\n",
      "Iteration 86, loss = 0.02257244\n",
      "Iteration 87, loss = 0.02203581\n",
      "Iteration 88, loss = 0.02201884\n",
      "Iteration 89, loss = 0.02209204\n",
      "Iteration 90, loss = 0.02144690\n",
      "Iteration 91, loss = 0.02117337\n",
      "Iteration 92, loss = 0.02146636\n",
      "Iteration 93, loss = 0.02135661\n",
      "Iteration 94, loss = 0.02085489\n",
      "Iteration 95, loss = 0.02035840\n",
      "Iteration 96, loss = 0.02023296\n",
      "Iteration 97, loss = 0.01979452\n",
      "Iteration 98, loss = 0.02007492\n",
      "Iteration 99, loss = 0.01936244\n",
      "Iteration 100, loss = 0.01917613\n",
      "Iteration 101, loss = 0.01913305\n",
      "Iteration 102, loss = 0.01892351\n",
      "Iteration 103, loss = 0.01865424\n",
      "Iteration 104, loss = 0.01852047\n",
      "Iteration 105, loss = 0.01838389\n",
      "Iteration 106, loss = 0.01812138\n",
      "Iteration 107, loss = 0.01796269\n",
      "Iteration 108, loss = 0.01824704\n",
      "Iteration 109, loss = 0.01770006\n",
      "Iteration 110, loss = 0.01767522\n",
      "Iteration 111, loss = 0.01745005\n",
      "Iteration 112, loss = 0.01797964\n",
      "Iteration 113, loss = 0.01714248\n",
      "Iteration 114, loss = 0.01698374\n",
      "Iteration 115, loss = 0.01685807\n",
      "Iteration 116, loss = 0.01685010\n",
      "Iteration 117, loss = 0.01667320\n",
      "Iteration 118, loss = 0.01650358\n",
      "Iteration 119, loss = 0.01628399\n",
      "Iteration 120, loss = 0.01649908\n",
      "Iteration 121, loss = 0.01607283\n",
      "Iteration 122, loss = 0.01598860\n",
      "Iteration 123, loss = 0.01597411\n",
      "Iteration 124, loss = 0.01600337\n",
      "Iteration 125, loss = 0.01562547\n",
      "Iteration 126, loss = 0.01580867\n",
      "Iteration 127, loss = 0.01542731\n",
      "Iteration 128, loss = 0.01548089\n",
      "Iteration 129, loss = 0.01518627\n",
      "Iteration 130, loss = 0.01511508\n",
      "Iteration 131, loss = 0.01496997\n",
      "Iteration 132, loss = 0.01508187\n",
      "Iteration 133, loss = 0.01487369\n",
      "Iteration 134, loss = 0.01482211\n",
      "Iteration 135, loss = 0.01475462\n",
      "Iteration 136, loss = 0.01451385\n",
      "Iteration 137, loss = 0.01437197\n",
      "Iteration 138, loss = 0.01425964\n",
      "Iteration 139, loss = 0.01417154\n",
      "Iteration 140, loss = 0.01431718\n",
      "Iteration 141, loss = 0.01400329\n",
      "Iteration 142, loss = 0.01389695\n",
      "Iteration 143, loss = 0.01386998\n",
      "Iteration 144, loss = 0.01372744\n",
      "Iteration 145, loss = 0.01365244\n",
      "Iteration 146, loss = 0.01353586\n",
      "Iteration 147, loss = 0.01340570\n",
      "Iteration 148, loss = 0.01341142\n",
      "Iteration 149, loss = 0.01328351\n",
      "Iteration 150, loss = 0.01322560\n",
      "Iteration 151, loss = 0.01306638\n",
      "Iteration 152, loss = 0.01307064\n",
      "Iteration 153, loss = 0.01294852\n",
      "Iteration 154, loss = 0.01288648\n",
      "Iteration 155, loss = 0.01276943\n",
      "Iteration 156, loss = 0.01276215\n",
      "Iteration 157, loss = 0.01256907\n",
      "Iteration 158, loss = 0.01253199\n",
      "Iteration 159, loss = 0.01251434\n",
      "Iteration 160, loss = 0.01237856\n",
      "Iteration 161, loss = 0.01236198\n",
      "Iteration 162, loss = 0.01236813\n",
      "Iteration 163, loss = 0.01224836\n",
      "Iteration 164, loss = 0.01219305\n",
      "Iteration 165, loss = 0.01212745\n",
      "Iteration 166, loss = 0.01223618\n",
      "Iteration 167, loss = 0.01199394\n",
      "Iteration 168, loss = 0.01197374\n",
      "Iteration 169, loss = 0.01198539\n",
      "Iteration 170, loss = 0.01172737\n",
      "Iteration 171, loss = 0.01174822\n",
      "Iteration 172, loss = 0.01161990\n",
      "Iteration 173, loss = 0.01154969\n",
      "Iteration 174, loss = 0.01143540\n",
      "Iteration 175, loss = 0.01143983\n",
      "Iteration 176, loss = 0.01138802\n",
      "Iteration 177, loss = 0.01195954\n",
      "Iteration 178, loss = 0.01149361\n",
      "Iteration 179, loss = 0.01125953\n",
      "Iteration 180, loss = 0.01110830\n",
      "Iteration 181, loss = 0.01106490\n",
      "Iteration 182, loss = 0.01100716\n",
      "Iteration 183, loss = 0.01097433\n",
      "Iteration 184, loss = 0.01085247\n",
      "Iteration 185, loss = 0.01084798\n",
      "Iteration 186, loss = 0.01078047\n",
      "Iteration 187, loss = 0.01080400\n",
      "Iteration 188, loss = 0.01066969\n",
      "Iteration 189, loss = 0.01068222\n",
      "Iteration 190, loss = 0.01065586\n",
      "Iteration 191, loss = 0.01051995\n",
      "Iteration 192, loss = 0.01047615\n",
      "Iteration 193, loss = 0.01040750\n",
      "Iteration 194, loss = 0.01037651\n",
      "Iteration 195, loss = 0.01038089\n",
      "Iteration 196, loss = 0.01023229\n",
      "Iteration 197, loss = 0.01017848\n",
      "Iteration 198, loss = 0.01014074\n",
      "Iteration 199, loss = 0.01006419\n",
      "Iteration 200, loss = 0.00995747\n",
      "Iteration 201, loss = 0.00993180\n",
      "Iteration 202, loss = 0.00982349\n",
      "Iteration 203, loss = 0.00986442\n",
      "Iteration 204, loss = 0.00991117\n",
      "Iteration 205, loss = 0.00969246\n",
      "Iteration 206, loss = 0.01000313\n",
      "Iteration 207, loss = 0.00985500\n",
      "Iteration 208, loss = 0.00969460\n",
      "Iteration 209, loss = 0.00957350\n",
      "Iteration 210, loss = 0.00946481\n",
      "Iteration 211, loss = 0.00945113\n",
      "Iteration 212, loss = 0.00938479\n",
      "Iteration 213, loss = 0.00932359\n",
      "Iteration 214, loss = 0.00928163\n",
      "Iteration 215, loss = 0.00922813\n",
      "Iteration 216, loss = 0.00915789\n",
      "Iteration 217, loss = 0.00924313\n",
      "Iteration 218, loss = 0.00917175\n",
      "Iteration 219, loss = 0.00907267\n",
      "Iteration 220, loss = 0.00902846\n",
      "Iteration 221, loss = 0.00897860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_iter=500, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test our model with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
